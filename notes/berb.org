#+TITLE: Concurrence selon https://berb.github.io/diploma-thesis/original/023_concurrency.html
#+AUTHOR: Dalker (daniel.kessler@dalker.org)
* Source
  https://berb.github.io/diploma-thesis/original/0_preface.html
  Thèse de diplôme de Benjamin Erb, Université d'Ulm, 2012

  Ce qui suit sont des notes prises en juin 2021 par Dalker principalement à
  partir de cette source.
* Buts de la concurrence et remarques générales
  - *réduire la latence* en subdivisant en parties exécutées de manière
    concurrente
  - *cacher la latence* lorsque des tâches bloquent en attendant du I/O ou du
    réseau
  - *augmenter le 'throughput'* en général ce qui contribue aussi aux tâches
    séquentielles qui attendent
  - la concurrence est intrinsèque à tout système distribué (processus dans
    machines séparées faisant partie d'un même système)
  - c'est le cas de toutes les applications web, qui peuvent être utilisées par
    plusieurs utilisateurs simultanément
* 4 approches pour programmer la concurrence
  - *séquentielle*: pas de concurrence, ordre total des opérations, peut garder
    déterminisme; possibilité de *coroutines*
  - *déclarative*: le flux de contrôle est implicite; non-déterminisme à
    l'exécution mais pas observable de l'extérieur
  - *messages*: les activités n'interragissent que par messages (synchrones ou
    asynchrones) et sinon sont isolées
  - *état partagé*: des ressources et états sont accessibles par plusieurs
    activités, ce qui nécessite des mécanismes de synchronisation et de
    coordination entre ces activités, sans quoi on pert tout déterminisme,
    invariants, cohérence et validité des états
* contrôle par synchronisation et coordination
  - deux soucis: accès à mêmes espaces mémoire, communication sur progression
    d'activités travaillant sur un même problème
  - *synchronisation* ("compétitive"): contrôle d'accès à ressources partagées entre activités différentes
  - *coordination* (= synchrnisation coopérative): collaboration entre activités
  - ces deux mécanismes peuvent être explicites ou implicites (cachés dans la
    sémantique du langage de programmation)
* modes d'exécution des tâches concurrentes
  - *multitasking* de l'OS pour gérer les tâches "interleaved" et alternées
  - *multiprocessing*: implémentation possible du multitasking avec plusieurs
    coeurs de cpu
  - *scheduling*: organisation de l'assignement de temps de processeur aux
    tâches par l'OS, suivant différentes stratégies
  - *preemption*: le scheduler assigne du temps à une tâche mais peut le
    révoquer, sans que la tâche n'ait de contrôle là-dessus
  - modèle *coopératif*: la tâche elle-même est responsable de "yield" après un
    certain temps
  - les *threads* d'un même processus sont comme des "processus légers" bon
    marché à créer, partageant de la mémoire, accès fichiers et autres
    ressources à l'interne du processus - ceci est peut-être invisible dans le
    langage et s'applique au "runtime"
* soucis particuliers des systèmes distribués
  - *fault tolerance*: il faut prévoir qu'un élément d'un réseau peut être
    inaccessible ou en panne
  - intégration des aspects distribués
  - des langages généralistes comme Java ne sont pas forcément adaptés à ce
    contexte
* modèle 1: concurrence basée sur Threads, Locks et Shared state
  - c'est compliqué et source d'erreurs, mais relativement proche de la
    programmation impérative "à la von Neumann", donc présent dans la plupart
    des langages de programmation
  - un *thread* est un flux de contrôle séquentiel apparamment isolé des autres
    activités *mais* à la différence d'un processus il partage des variables et
    états au sein de son processus
  - la *programmation séquentielle* est basée sur le concept d'*état mutable*,
    donc les threads peuvent être en compétition pour l'écriture!
  - on utilise généralement du *preemptive scheduling*, donc l'interleaving
    exact n'est pas connu d'avance, ce qui crée de l'indétermination
  - état mutable + indétermination => danger de "race conditions" quand
    plusieurs threads sont en compétition pour l'accès à une *section critique*
    -> on a besoin de *locking mechanisms* pour éviter des états parallèles ou incohérents
  - le *sémaphore* de Dijkstra (1964) utilise des fonctions /wait/ pour entrer dans une
    section critique et /signal/ pour la libérer à la fin
  - le *monitor* de Hoare (1974) protège des sections (généralement des objets
    ou fonctions/méthodes) avec des *variables de condition*
  - cependant, la solution (les locks) peut créer un nouveau problème si les
    verrous ne sont pas déverrouillés ou si on arrive à un *deadlock*, par
    exemple avec des dépendances cycliques entre verrous
  - les *livelocks* ou *starvations* créent une réaction cyclique empêchant à un
    thread d'acquérir un verrou
  - c'est pire avec beaucoup de petits verrous - mais un seul gros verrou réduit
    le souhait de "parallélisme"...
  - l'utilisation *directe* des threads et verrous met le programmeur devant
    tous ces problèmes, donc leur utilisation *indirecte* ("cachée" par derrière
    d'autres mécanismes) est fortement recommandée
* modèle 2: concurrence à état partagé avec mémoire transactionnelle
  - la mémoire transactionnelle donne un accès "haut-niveau" et sûr aux verrous
  - une *transaction* est un concept initialement des /bases de donnée/,
    appliquée à la *concurrence avec état partagé*
  - une transaction doit garantir l'atomicité, cohérence, isolation et
    durabilité, en donnant l'impression d'être une opération unique
  - la gestion des transactions peut être "pessimiste": verrous forts, peu de
    transactions à la fois ou "optimiste": vérifications faites à la fin
    uniquement pouvant résulter en l'annulation et redémarrage de la
    transaction, ce qui a normalement une nettement meilleure performance
  - les implémentations de "software transactional memory" permettent le
    contrôle de concurrence "optimiste"
  - le langage de programmation doit permettre d'indiquer que des sections de
    code sont "transactionnelles", et de distinguer entre variables
    "transactionnelles" et simples variables locales sans danger
  - des mécanismes de "retry" et "orElse" sont possibles dans certains langages,
    pour prévoir des situations en cours de transactions concurrentes
  - limitation: c'est bien pour la mémoire partagée, mais ça ne gère pas le
    partage de ressources extérieures
  - limitation: les transactions ne peuvent pas avoir d'effets de bord au-delà
    de leur modification de l'état partagé déclaré comme "transactionnel", donc
    pas d'accès I/O par exemple
  - souci: la "famine" peut toujours arriver! (compétition entre transactions)
  - implémentation: le langage *Clojure* se spécialise sur ce paradigme de
    concurrence; clojure est un langage basé sur Lisp et tournant sur une JVM,
    avec un concept fort d'*immutabilité* et des agents asynchrones
  - les mécanismes ressemblent à ceux du "garbage collector" et ont un but
    semblable de décharger le programmeur de la gestion minutieuse de la mémoire
  - Clojure de plus près: on a des *valeurs* immutables, des *identités*
    pointant vers une valeur de manière mutable, des *références* vers des
    identités, et des *états* dont le changement se fait en réaffectant des
    identités à des nouvelles valeurs, le tout grâce à des structures de donnée
    persistantes (au sens de "gardant leur histoire")
* état partagé ou pas d'état partagé?
  Changement de paradigme par rapport aux modèles précédents: il n'y a plus
  d'état partagé, donc plus besoin de le protéger via des verrous ou des
  transactions.
* modèle 3: concurrence basée sur Acteurs
  - origine: modèles de concurrence de Hewitt (1973) et concpts de messages de
    Hoare (1978)
  - les *acteurs* sont les *primitives concurrentes* qui peuvent:
    1. envoyer un nombre fini de messages aux autres acteurs
    2. créer un nombre fini de nouveaux acteurs
    3. changer son état interne, avec effet lors de la prochaine réception de
       message
  - les messages sont transmis de manière *asynchrone*, sans entités
    intermédiaire (pas de "canal") mais via adressage à une "boîte postale"
    un acteur peut avoir 0, 1 ou plusieurs "boîtes postales"; un message peut
    même être adressé d'un acteur à lui-même via une de ses boîtes
  - il n'y a aucune garantie sur l'ordre de réception des messages ou leur temps
    de trajet
  - les "race conditions" ne sont pas possibles: une "boîte aux lettres" ne peut
    que recevoir (push) et être consultée (pop) des messages, ce qui ne crée
    jamais de conflit
  - historiquement, *Erlang* a été le premier langage a implémenter la
    concurrence selon ce modèle
  - de nos jours, certains langages ont des primitives selon ce modèle ou des
    librairies implémentant ce modèle comme une couche au-dessus du
    multithreading (c'est le cas par exemple de Scala: akka.actor.Actor)
  - mise en garde: si on utilise ce modèle, il faut fortement se conformer à
    l'absence d'état partagé, donc ne pas envoyer de références ou pointeurs
    dans les messages, mais uniquement des données immuables et des adresses de
    boîtes aux lettres d'acteurs
  - dans la pratique, deux messages provenant d'un même acteur vers un même
    destinataire arrivent dans le même ordre dans la plupart des
    implémentations, mais des messages d'acteurs différents n'ont pas d'ordre
    garanti (ça dépend du "interleaving")
  - ce modèle est très facile à étendre au cas d'un système distribué
  - si un acteur "crashe", cela n'affecte pas les autres ("fault tolerance"); on
    peut gérer ces situations avec des "acteurs superviseurs", un "superviseur"
    peut alors recréer l'acteur, stoper d'autres acteurs ou signaler l'erreur à
    son propre "superviseur"
  - il peut encore y avoir un "deadlock" si deux acteurs attendent chacun un
    message de l'autre; on évite cela en utilisant des "timeouts"
  - si on s'y prend mal et ne prend pas en compte l'asynchronie intrinsèque au
    modèle, l'ordre arbitraire d'arrivée des messages pourrait donner
    l'impression d'une "race condition"
  - en implémentation, un acteur est plus "léger" qu'un thread, donc on peut
    sans problème en créer beaucoup
  - la coordination des acteurs peut être compliquée (chacun a son propre état
    isolé); certaines implémentations le permettent à un plus "haut niveau" via
    une stratégie d'échange de messages de coordination, par example via un
    "transactor", acteur qui émule des opérations "transactionnelles" entre
    plusieurs acteurs, ce qui se déroule un peu comme dans le modèle "Shared
    Transaction Memory"
* modèle 4: concurrence basée sur Events
  - Le modèle avec Events + event-handlers est semblable à celui d'Acteurs +
    messages, mais ce n'est pas tout à fait la même chose. Le modèle basé sur
    Events est moins contraignant et a pour but essentiel de se débarasser des
    "call stacks" grâce aux messages.
  - "call stack" normal: quand une fonction/méthode est appelée, l'appelant
    attend le retour de l'appel avant de récupérer son contexte et continuer son
    opération; un programme est alors une suite d'instructions et appels de
    fonctions liés par le "call stack"
  - avec une architecture "event-driven", il n'y a pas de "call stack"! on n'a
    plus d'appel/retour de fonction au sens classique; à la place on a des
    "événements", d'origine interne ou externe au programme; le "lanceur
    d'événement" ne sait pas qui va traiter ("handle") cet événement; comme dans
    le modèle acteur, ce paradigme est intrinsèquement *asynchrone*
  - une implémentation courante est avec un "event loop" et des "event handlers"
    basés sur des threads, mais on peut aussi faire du "single-thread"
  - dans une implémentation d'event-loop single-thread, on n'a pas besoin de
    locks parce qu'il n'y a aucun accès concurrent à des données, ce qui se
    combine bien avec des accès I/O asynchrones non-bloquants (requêtes envoyées
    périodiquement à l'OS pour vérifier si la ressource est accessible); c'est
    efficace pour de la concurrence I/O
  - si en plus on impose de ne rien partager (comme dans le modèle Acteur) c'est
    facile à paralléliser
  - l'objet de base est la *fonction de callback* enregistrée pour traiter un
    certain événement; ces fonctions doivent être courtes (ne pas prendre trop
    de temps cpu) pour ne pas bloquer l'event-loop; typiquement elles lancent
    d'autres événements et possiblement lancent d'autres opérations en
    arrière-plan
  - les fonctions anonymes et les closures sont très utiles en event-driven
    programming: les premières pour définir facilement des callbacks, les
    secondes à la place du contexte, pour fournir une continuité (le callback
    défini comme une closure contient son propre contexte caché)
  - en single-thread event handler, un seul callback est effectué à la fois, ce
    qui évite tous les problèmes de conflits (pas de deadlock possible) - mais
    attention à respecter le fait que les callbacks soient rapides, sinon on
    peut avoir l'équivalent de la famine; de plus, il ne faut jamais présupposer
    quoi que ce soit sur l'ordre de traitement des callbacks
  - ccl: c'est très efficace pour des situations "I/O"; pour du cpu-intensif, il
    faut "dispatcher" des "workers" (dans des threads séparés) pour ne pas tout
    bloquer
* autres approches et primitives pour la concurrence
** futures, promises et asynchrnous tasks
   - Ces mécanismes permetent de "dispatcher" une tâche computationnelle
     indépendante puis continuer le flux de contrôle normal, et synchroniser le
     résultat plus tard quand il est disponible. Ils fournissent un "objet
     proxy" qui permet de demander si le résultat est disponible et l'acquérir
     si c'est le cas
   - différence entre les deux selon
     https://stackoverflow.com/a/28821051/613191:
     le Future est un contenant "read-only" pour un résultat qui n'existe pas
     encore, tandis qu'un Promise peut être écrit (une seule fois en
     principe). Dans certains langages (Scala, C++) ces objets sont clairement
     complémentaires (un pour le "client", l'autre pour le "serveur")
   - dans le modèle Acteur, un Future peut représenter le fait d'envoyer un
     message à un autre acteur et attendre sa réponse
   - dans le modèle Event-based, les events remplacent les Futures
   - le "scatter-gather" pattern utilise des futures pour paralléliser des
     calculs et rassembler les résultats plus tard
** coroutines, fibers et green threads
   - Les coroutines, fibres et "green threads" sont des généralisation des
     sous-routines.
   - Une /sous-routine/ s'exécute séquentiellement, tout de suite, tandis qu'une
     /coroutine/ peut être suspendue et reprendre son exécution à différents
     moments selon le besoin. Ils servent d'alternative aux /threads/ pour
     implémenter "bas-niveau" des modèles de concurrence de plus "haut-niveau"
     (par exemple des Actors ou Events)
   - le langage /Google Go/ utilise ce mécanisme, avec ses "Goroutines"; au
     niveau "runtime" elles sont automatiquement mappées vers un certain nombre
     de threads - ou pas, selon le besoin (elles vont donc au-delà d'une
     coroutine conventionnelle)
** channels et synchronous message passing
   - les messages des modèle Acteur et Event sont asynchrones, mais hors de ces
     modples on peut aussi utiliser des /Messages Synchrones/: l'expériteur et
     destinataire doivent être tous deux prêts et sont bloqués pendant la
     transmission. Cela fournit automatiquement une forme de /synchronisation/.
   - la messagerie synchrone utilise des /canaux/ explicites entre entités
     anonymes
   - Google Go utilise des canaux, un peu comme les "pipes" des shell
     Unix/Linux, pour des échanges entre /goroutines/.
** dataflow
   - il s'agit de /concurrence déclarative/, élégante mais peu commune en
     pratique.
   - on part de relations entre opérations, ce qui équivaut à un /graphe de
     dépendances/ pour le flux d'exécution, permettant au runtime de
     paralléliser ce qui est possible de manière automatisée et synchroniser
     quand c'est nécessaire, via des /channels/ par exemple
    
